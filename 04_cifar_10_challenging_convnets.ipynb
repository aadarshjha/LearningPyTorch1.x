{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_cifar-10_challenging_convnets.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/LearningPyTorch1.x/blob/master/04_cifar_10_challenging_convnets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOzoRVnkWAG_",
        "colab_type": "text"
      },
      "source": [
        "### CIFAR-10: A More Challenging Dataset for CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GacP-bUnoyn7",
        "colab_type": "text"
      },
      "source": [
        "(Google has upgraded to pytorch 1.2.0 and torchvision 0.4.0, but since we've been using pytorch 1.1.0 and torchvision 0.3.0, we'll install those version and upgrade to the new ones at some point in the future)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP4oGMsKgIXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch==1.1.0 torchvision==0.3.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OnMQq70UEsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import statistics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6dd5QSJcj49",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5bd8f389-1965-4916-a882-7155c64f31cd"
      },
      "source": [
        "print('PyTorch version:', torch.__version__)\n",
        "print('Torchvision version:', torchvision.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version: 1.1.0\n",
            "Torchvision version: 0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OApIwQsWwTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_channels, num_classes):\n",
        "        super(BasicCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_channels, 32, 3, stride=1, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=0)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1, padding=0)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(4*4*64, 64, bias=True)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        x = F.relu(self.conv1(X))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.reshape(-1, 4*4*64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apBUjbV6Zsa-",
        "colab_type": "code",
        "outputId": "04ae3f68-9d04-461c-c1b1-0557634352ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "cuda = torch.device('cuda')\n",
        "\n",
        "model = BasicCNN(3, 10)\n",
        "model.to(cuda)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BasicCNN(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=1024, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnMP-MCOZ0Ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_set = CIFAR10('./cifar10', train=True, download=True, transform=dataset_transform)\n",
        "valid_set = CIFAR10('./cifar10', train=False, download=True, transform=dataset_transform)\n",
        "\n",
        "print(train_set.data.shape)\n",
        "print(valid_set.data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sigam0A2mucV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_set, batch_size=128, num_workers=0, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=512, num_workers=0, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jINuk6sDnAtE",
        "colab_type": "code",
        "outputId": "4fd3e544-773c-471a-c857-c2c34eef7824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = torch.randn(128, 3, 32, 32, device=cuda)\n",
        "output = model(x)\n",
        "print(output.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WEcZRKUAJeX",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "**Optimizer: Stochastic Gradient Descent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXbrWUzRnozL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOybIMl-AZ15",
        "colab_type": "text"
      },
      "source": [
        "**Train function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6cNzTpxnxh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    \n",
        "    train_batch_losses = []\n",
        "    \n",
        "    for batch, labels in train_loader:\n",
        "        batch = batch.to(cuda)\n",
        "        labels = labels.to(cuda)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(batch)\n",
        "        loss = loss_fn(y_pred, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_batch_losses.append(float(loss))\n",
        "        \n",
        "        mean_loss = statistics.mean(train_batch_losses)\n",
        "        \n",
        "    return mean_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TXWNPNsAc_m",
        "colab_type": "text"
      },
      "source": [
        "**Validation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gymzrcxAgGJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, loss_fn, optimizer):\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        validation_batch_losses = []\n",
        "        \n",
        "        for batch, labels in valid_loader:\n",
        "            batch = batch.to(cuda)\n",
        "            labels = labels.to(cuda)\n",
        "            \n",
        "            labels_pred = model(batch)\n",
        "            loss = loss_fn(labels_pred, labels)\n",
        "            \n",
        "            validation_batch_losses.append(float(loss))\n",
        "            \n",
        "            mean_loss = statistics.mean(validation_batch_losses)\n",
        "            \n",
        "    return mean_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFZvUaTiAhN_",
        "colab_type": "text"
      },
      "source": [
        "**Accuracy function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YczNCGVTJXp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(model, loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch, labels in loader:\n",
        "            batch = batch.to(cuda)\n",
        "            labels = labels.to(cuda)\n",
        "        \n",
        "            labels_pred = model(batch)\n",
        "        \n",
        "            _, predicted = torch.max(labels_pred.data, 1)\n",
        "        \n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    return (100 * correct / total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsekLWQMApUZ",
        "colab_type": "text"
      },
      "source": [
        "### Training our Convnet on CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_369wY6ILNgg",
        "colab_type": "code",
        "outputId": "7be9fd95-f4aa-497a-9ef8-5f61c0d67142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(1, 1+15):\n",
        "    \n",
        "    print('Epoch: ', epoch)\n",
        "    \n",
        "    train_loss = train(model, loss_fn, optimizer)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    print('Training loss:', train_loss)\n",
        "    print('Training accuracy: {}%'.format(accuracy(model, train_loader)))\n",
        "    \n",
        "    valid_loss = validate(model, loss_fn, optimizer)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print('Validation loss:', valid_loss)\n",
        "    print('Validation accuracy: {}%'.format(accuracy(model, valid_loader)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "Training loss: 2.1581876902933925\n",
            "Training accuracy: 28.622%\n",
            "Validation loss: 1.9237818956375121\n",
            "Validation accuracy: 28.97%\n",
            "Epoch:  2\n",
            "Training loss: 1.7154211732737548\n",
            "Training accuracy: 43.212%\n",
            "Validation loss: 1.531798893213272\n",
            "Validation accuracy: 43.02%\n",
            "Epoch:  3\n",
            "Training loss: 1.5529533161226745\n",
            "Training accuracy: 47.304%\n",
            "Validation loss: 1.475709730386734\n",
            "Validation accuracy: 46.23%\n",
            "Epoch:  4\n",
            "Training loss: 1.451710115003464\n",
            "Training accuracy: 51.204%\n",
            "Validation loss: 1.4317897140979767\n",
            "Validation accuracy: 48.95%\n",
            "Epoch:  5\n",
            "Training loss: 1.3625466113200273\n",
            "Training accuracy: 55.386%\n",
            "Validation loss: 1.3580244183540344\n",
            "Validation accuracy: 52.0%\n",
            "Epoch:  6\n",
            "Training loss: 1.3126608648568467\n",
            "Training accuracy: 57.906%\n",
            "Validation loss: 1.3264195144176483\n",
            "Validation accuracy: 54.59%\n",
            "Epoch:  7\n",
            "Training loss: 1.250039939685246\n",
            "Training accuracy: 60.592%\n",
            "Validation loss: 1.256823229789734\n",
            "Validation accuracy: 57.06%\n",
            "Epoch:  8\n",
            "Training loss: 1.2026749664865186\n",
            "Training accuracy: 55.582%\n",
            "Validation loss: 1.4108488321304322\n",
            "Validation accuracy: 51.95%\n",
            "Epoch:  9\n",
            "Training loss: 1.1989467331515553\n",
            "Training accuracy: 59.03%\n",
            "Validation loss: 1.3630026817321776\n",
            "Validation accuracy: 54.37%\n",
            "Epoch:  10\n",
            "Training loss: 1.164682947918582\n",
            "Training accuracy: 62.582%\n",
            "Validation loss: 1.2806936979293824\n",
            "Validation accuracy: 57.21%\n",
            "Epoch:  11\n",
            "Training loss: 1.1828153855965267\n",
            "Training accuracy: 59.708%\n",
            "Validation loss: 1.3839815557003021\n",
            "Validation accuracy: 54.23%\n",
            "Epoch:  12\n",
            "Training loss: 1.134396767219924\n",
            "Training accuracy: 62.538%\n",
            "Validation loss: 1.3242146611213683\n",
            "Validation accuracy: 55.9%\n",
            "Epoch:  13\n",
            "Training loss: 1.13854092374787\n",
            "Training accuracy: 62.27%\n",
            "Validation loss: 1.3485555469989776\n",
            "Validation accuracy: 55.86%\n",
            "Epoch:  14\n",
            "Training loss: 1.1381533726706834\n",
            "Training accuracy: 63.112%\n",
            "Validation loss: 1.3471283912658691\n",
            "Validation accuracy: 56.18%\n",
            "Epoch:  15\n",
            "Training loss: 1.1416207033654917\n",
            "Training accuracy: 58.55%\n",
            "Validation loss: 1.4932811081409454\n",
            "Validation accuracy: 51.74%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}