{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_cifar-10_challenging_convnets.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/LearningPyTorch1.x/blob/master/04_cifar_10_challenging_convnets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOzoRVnkWAG_",
        "colab_type": "text"
      },
      "source": [
        "### CIFAR-10: A More Challenging Dataset for CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OnMQq70UEsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import statistics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OApIwQsWwTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_channels, num_classes):\n",
        "        super(BasicCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_channels, 32, 3, stride=1, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=0)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1, padding=0)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(4*4*64, 64, bias=True)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        x = F.relu(self.conv1(X))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.reshape(-1, 4*4*64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apBUjbV6Zsa-",
        "colab_type": "code",
        "outputId": "93e59d87-acf2-435f-c42c-f2792ebfb04d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "cuda = torch.device('cuda')\n",
        "\n",
        "model = BasicCNN(3, 10)\n",
        "model.to(cuda)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BasicCNN(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=1024, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnMP-MCOZ0Ue",
        "colab_type": "code",
        "outputId": "25f9557b-a51e-44a4-e669-5f0677d96b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "dataset_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_set = CIFAR10('./cifar10', train=True, download=True, transform=dataset_transform)\n",
        "valid_set = CIFAR10('./cifar10', train=False, download=True, transform=dataset_transform)\n",
        "\n",
        "print(train_set.data.shape)\n",
        "print(valid_set.data.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/170498071 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:01, 86022492.04it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifar10/cifar-10-python.tar.gz to ./cifar10\n",
            "Files already downloaded and verified\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sigam0A2mucV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_set, batch_size=128, num_workers=0, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=512, num_workers=0, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jINuk6sDnAtE",
        "colab_type": "code",
        "outputId": "435202f5-23cd-4b52-962f-4cc1c826b33c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = torch.randn(128, 3, 32, 32, device=cuda)\n",
        "output = model(x)\n",
        "print(output.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXbrWUzRnozL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6cNzTpxnxh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    \n",
        "    train_batch_losses = []\n",
        "    \n",
        "    for batch, labels in train_loader:\n",
        "        batch = batch.to(cuda)\n",
        "        labels = labels.to(cuda)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(batch)\n",
        "        loss = loss_fn(y_pred, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_batch_losses.append(float(loss))\n",
        "        \n",
        "        mean_loss = statistics.mean(train_batch_losses)\n",
        "        \n",
        "    return mean_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gymzrcxAgGJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, loss_fn, optimizer):\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        validation_batch_losses = []\n",
        "        \n",
        "        for batch, labels in valid_loader:\n",
        "            batch = batch.to(cuda)\n",
        "            labels = labels.to(cuda)\n",
        "            \n",
        "            labels_pred = model(batch)\n",
        "            loss = loss_fn(labels_pred, labels)\n",
        "            \n",
        "            validation_batch_losses.append(float(loss))\n",
        "            \n",
        "            mean_loss = statistics.mean(validation_batch_losses)\n",
        "            \n",
        "    return mean_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YczNCGVTJXp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(model, loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch, labels in loader:\n",
        "            batch = batch.to(cuda)\n",
        "            labels = labels.to(cuda)\n",
        "        \n",
        "            labels_pred = model(batch)\n",
        "        \n",
        "            _, predicted = torch.max(labels_pred.data, 1)\n",
        "        \n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    return (100 * correct / total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_369wY6ILNgg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef049916-6b69-4703-cfe0-0dde54ccf4c1"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(1, 1+15):\n",
        "    \n",
        "    print('Epoch: ', epoch)\n",
        "    \n",
        "    train_loss = train(model, loss_fn, optimizer)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    print('Training loss:', train_loss)\n",
        "    print('Training accuracy: {}%'.format(accuracy(model, train_loader)))\n",
        "    \n",
        "    valid_loss = validate(model, loss_fn, optimizer)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print('Validation loss:', valid_loss)\n",
        "    print('Validation accuracy: {}%'.format(accuracy(model, valid_loader)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "Training loss: 1.1408028633088407\n",
            "Training accuracy: 65.464%\n",
            "Validation loss: 1.2514345467090606\n",
            "Validation accuracy: 58.61%\n",
            "Epoch:  2\n",
            "Training loss: 1.1092442223787917\n",
            "Training accuracy: 63.538%\n",
            "Validation loss: 1.3400908768177033\n",
            "Validation accuracy: 55.2%\n",
            "Epoch:  3\n",
            "Training loss: 1.119702333227143\n",
            "Training accuracy: 64.53%\n",
            "Validation loss: 1.345867383480072\n",
            "Validation accuracy: 55.97%\n",
            "Epoch:  4\n",
            "Training loss: 1.1152159844518013\n",
            "Training accuracy: 61.222%\n",
            "Validation loss: 1.4285688281059266\n",
            "Validation accuracy: 53.75%\n",
            "Epoch:  5\n",
            "Training loss: 1.0982776875691036\n",
            "Training accuracy: 66.952%\n",
            "Validation loss: 1.3548159956932069\n",
            "Validation accuracy: 57.51%\n",
            "Epoch:  6\n",
            "Training loss: 1.1000229629409282\n",
            "Training accuracy: 61.238%\n",
            "Validation loss: 1.4709051728248597\n",
            "Validation accuracy: 53.16%\n",
            "Epoch:  7\n",
            "Training loss: 1.1492690730582722\n",
            "Training accuracy: 62.446%\n",
            "Validation loss: 1.3778671860694884\n",
            "Validation accuracy: 54.4%\n",
            "Epoch:  8\n",
            "Training loss: 1.1143412532099068\n",
            "Training accuracy: 64.096%\n",
            "Validation loss: 1.4010924398899078\n",
            "Validation accuracy: 55.57%\n",
            "Epoch:  9\n",
            "Training loss: 1.1654106294712447\n",
            "Training accuracy: 64.328%\n",
            "Validation loss: 1.4029450416564941\n",
            "Validation accuracy: 55.87%\n",
            "Epoch:  10\n",
            "Training loss: 1.1244221323591364\n",
            "Training accuracy: 64.906%\n",
            "Validation loss: 1.4268757104873657\n",
            "Validation accuracy: 56.41%\n",
            "Epoch:  11\n",
            "Training loss: 1.11390764954145\n",
            "Training accuracy: 66.176%\n",
            "Validation loss: 1.4517214000225067\n",
            "Validation accuracy: 55.99%\n",
            "Epoch:  12\n",
            "Training loss: 1.1193567833022389\n",
            "Training accuracy: 63.582%\n",
            "Validation loss: 1.4099547684192657\n",
            "Validation accuracy: 55.07%\n",
            "Epoch:  13\n",
            "Training loss: 1.113428102887195\n",
            "Training accuracy: 62.828%\n",
            "Validation loss: 1.5568248450756073\n",
            "Validation accuracy: 54.04%\n",
            "Epoch:  14\n",
            "Training loss: 1.1263751229056922\n",
            "Training accuracy: 60.35%\n",
            "Validation loss: 1.537716794013977\n",
            "Validation accuracy: 53.03%\n",
            "Epoch:  15\n",
            "Training loss: 1.126500476809109\n",
            "Training accuracy: 66.256%\n",
            "Validation loss: 1.4302542984485627\n",
            "Validation accuracy: 57.14%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}