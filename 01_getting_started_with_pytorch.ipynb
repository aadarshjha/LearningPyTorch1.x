{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_getting_started_with_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/LearningPyTorch1.x/blob/master/01_getting_started_with_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsDBlN9rzm0E",
        "colab_type": "text"
      },
      "source": [
        "## Getting Started with PyTorch: Training a NN on MNIST\n",
        "\n",
        "This a small series of notebooks in which I introduce PyTorch, Facebook's machine learning framework. We start by training an artifical neural network on the MNIST dataset. In this example we are doing image classification, so we feed an image to the neural network and it outputs the probabilities of that image belongs to a particular class.\n",
        "\n",
        "We demonstrate the use of a GPU for training and validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZTfbDBH0XQ-",
        "colab_type": "text"
      },
      "source": [
        "Let's start by importing torch, which is the main library, torchvision and numpy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEMdXm--R0XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statistics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RK6E55_1ALO",
        "colab_type": "text"
      },
      "source": [
        "Pytorch is included by default in the Colab notebooks, but it's a good idea to check the installed version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crqFErSxekd7",
        "colab_type": "code",
        "outputId": "3ba3e5b3-cb4d-48cc-eabd-876a3871e1f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('PyTorch version:', torch.__version__)\n",
        "print('Torchvision version:', torchvision.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version: 1.1.0\n",
            "Torchvision version: 0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcLTpCU2jSVq",
        "colab_type": "text"
      },
      "source": [
        "### Simple Neural Network\n",
        "\n",
        "We will start with the basic example of a shallow NN: an input layer, a hidden layer and the output layer. We'll use dropout to avoid overfitting.\n",
        "\n",
        "Each MNIST training example consists of a 28x28 pixels image in grayscale (1 channel), that is turned into a 784-elements vector. The input layer has 784 neurons, and we have a hidden layer of 128 neurons. The output layer has 1 neuron for each one of the classes, in this case 10 neurons (10 digits - 0, 1 2, 3, etc).\n",
        "\n",
        "To implement our neural network, we create the class **BasicNN** and inherit the methods and properties from the Module class (**nn.Module**):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM-ForR-l6jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(BasicNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "        self.drop = nn.Dropout(0.2)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.reshape(-1, 28*28)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAw-H2RWAIAF",
        "colab_type": "text"
      },
      "source": [
        "We'll be using the GPU that is included in the Colab notebooks, so we create a pytorch device and send the model to the device:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRlAtGqH7IFx",
        "colab_type": "code",
        "outputId": "3b95f483-d926-466d-a7b2-5c168d454cd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "cuda = torch.device('cuda')\n",
        "\n",
        "model = BasicNN(28*28, 128, 10)\n",
        "model.to(cuda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BasicNN(\n",
              "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (drop): Dropout(p=0.2)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6tKMYL56u-f",
        "colab_type": "text"
      },
      "source": [
        "### MNIST\n",
        "\n",
        "The MNIST dataset is included in the torchvision module and it's really strighforward to download it. Before using MNIST we need to define a couple of transformations, which are map functions that are run through the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDj5nPWK7BjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.1307], [0.3081])\n",
        "])\n",
        "\n",
        "valid_transform = train_transform\n",
        "\n",
        "train_set = MNIST('./data/mnist', train=True, download=True, transform=train_transform)\n",
        "valid_set = MNIST('./data/mnist', train=False, download=True, transform=train_transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njadg57rBsZH",
        "colab_type": "text"
      },
      "source": [
        "Now let's take a look at the shape of our datasets; as you can see we have a training set with 60,000 images (28x28) and a validation/test set with 10,000 images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvs5FGUw4rpd",
        "colab_type": "code",
        "outputId": "e37d248b-b4a2-4ef8-ebb3-aea0fddc399a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train_set.data.shape)\n",
        "print(valid_set.data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([10000, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9e1v9mx6pgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_set, batch_size=128, num_workers=0, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=512, num_workers=0, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGLJonyAGDb2",
        "colab_type": "text"
      },
      "source": [
        "Now we create a 2D tensor of 28x28 with random elements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlTAX7xpBEYE",
        "colab_type": "code",
        "outputId": "f487ded1-942d-434d-aa35-99b106cd2820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input = torch.randn(28, 28, device=cuda)\n",
        "out = model(input)\n",
        "print(out.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it8hiwlF1IBA",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "**Optimizer: Stochastic Gradient Descent**\n",
        "\n",
        "Stochastic gradient descent is our choice for the optimizer. In pytorch we set the learning rate (lr) and momentum:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9_ek-1aKVgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
        "# Stochastic gradient descent\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYe_IHzc8AVR",
        "colab_type": "text"
      },
      "source": [
        "**Train function**\n",
        "\n",
        "The train function is fairly simple:\n",
        "\n",
        "(i) get a batch of training examples and send them to the GPU (CUDA)\n",
        "\n",
        "(ii) set all gradients to zero\n",
        "\n",
        "(iii) forward propagate the batch through the NN and calculate the loss\n",
        "\n",
        "(iv) backpropagate the loss through the NN and update its parameters (weights and biases)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpi2WywW1OcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, loss_fn, optimizer):\n",
        "    \n",
        "    # https://pytorch.org/docs/stable/nn.html#torch.nn.Module.train\n",
        "    # set the module in training mode\n",
        "    model.train()\n",
        "    \n",
        "    train_batch_losses = []\n",
        "    \n",
        "    for batch, labels in train_loader:\n",
        "        \n",
        "        #send the training data to the GPU\n",
        "        batch = batch.to(cuda)\n",
        "        labels = labels.to(cuda)\n",
        "        \n",
        "        #set all gradient to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #forward propagation\n",
        "        y_pred = model(batch)\n",
        "        \n",
        "        #calculate loss\n",
        "        loss = loss_fn(y_pred, labels)\n",
        "        \n",
        "        #backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        #update the parameters (weights and biases)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_batch_losses.append(float(loss))\n",
        "        \n",
        "        mean_loss = statistics.mean(train_batch_losses)\n",
        "    \n",
        "    return mean_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH97w_GO9apk",
        "colab_type": "text"
      },
      "source": [
        "**Validation function**\n",
        "\n",
        "Since we already have updated the NN's parameters, now we calculate the loss and eventually the accuracy for our validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCYCnM4aPoE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, loss_fn, optimizer):\n",
        "    \n",
        "    # set the model in validation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # save predictions for later\n",
        "    predictions = []\n",
        "    \n",
        "    # stop tracking the parameters for backpropagation\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        validation_batch_losses = []\n",
        "        \n",
        "        for batch, labels in valid_loader:\n",
        "            \n",
        "            # send the validation data to the GPU\n",
        "            batch = batch.to(cuda)\n",
        "            labels = labels.to(cuda)\n",
        "            \n",
        "            # forward propagation\n",
        "            labels_pred = model(batch)\n",
        "            \n",
        "            # calculate the loss\n",
        "            loss = loss_fn(labels_pred, labels)\n",
        "            \n",
        "            validation_batch_losses.append(float(loss))\n",
        "            \n",
        "            mean_loss = statistics.mean(validation_batch_losses)\n",
        "    \n",
        "    return mean_loss\n",
        "            \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_koVOUp9r31",
        "colab_type": "text"
      },
      "source": [
        "**Accuracy function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JvPfHnP9EYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(model, loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # set the model in validation mode to deactivate the dropout layer\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch, labels in loader:\n",
        "            batch = batch.to(cuda)\n",
        "            labels = labels.to(cuda)\n",
        "            \n",
        "            labels_pred = model(batch)\n",
        "            \n",
        "            _, predicted = torch.max(labels_pred.data, 1)\n",
        "            \n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            return (100 * correct / total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqPackQMjQq1",
        "colab_type": "text"
      },
      "source": [
        "**Training our Neural Network**\n",
        "\n",
        "Now we are ready to train our neural network. We start by choosing the cross entropy function as our loss function, and defining a couple of variables to keep track of the losses:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcxTN-cPzHY4",
        "colab_type": "code",
        "outputId": "7011db9d-a5bb-4679-d788-3ab65c286339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(1, 1+10):\n",
        "    \n",
        "    print('Epoch number ', epoch)\n",
        "    \n",
        "    train_loss = train(model, loss_fn, optimizer)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    print('Training loss:', train_loss)\n",
        "    print('Training accuracy: {}%'.format(accuracy(model, train_loader)))\n",
        "    \n",
        "    valid_loss = validate(model, loss_fn, optimizer)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print('Validation loss:', valid_loss)\n",
        "    print('Validation accuracy: {}%'.format(accuracy(model, valid_loader)))\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number  1\n",
            "Training loss: 0.325621415986054\n",
            "Training accuracy: 98.4375%\n",
            "Validation loss: 0.1631393177434802\n",
            "Validation accuracy: 96.2890625%\n",
            "Epoch number  2\n",
            "Training loss: 0.19709268558635387\n",
            "Training accuracy: 97.65625%\n",
            "Validation loss: 0.12608978673815727\n",
            "Validation accuracy: 96.6796875%\n",
            "Epoch number  3\n",
            "Training loss: 0.16697214499735502\n",
            "Training accuracy: 100.0%\n",
            "Validation loss: 0.12761372942477464\n",
            "Validation accuracy: 96.484375%\n",
            "Epoch number  4\n",
            "Training loss: 0.1504451731509809\n",
            "Training accuracy: 97.65625%\n",
            "Validation loss: 0.13551484160125254\n",
            "Validation accuracy: 98.2421875%\n",
            "Epoch number  5\n",
            "Training loss: 0.14310380744177903\n",
            "Training accuracy: 97.65625%\n",
            "Validation loss: 0.1313215672969818\n",
            "Validation accuracy: 96.875%\n",
            "Epoch number  6\n",
            "Training loss: 0.13251053442610605\n",
            "Training accuracy: 95.3125%\n",
            "Validation loss: 0.12676310073584318\n",
            "Validation accuracy: 97.4609375%\n",
            "Epoch number  7\n",
            "Training loss: 0.12066883703014616\n",
            "Training accuracy: 97.65625%\n",
            "Validation loss: 0.1483224864117801\n",
            "Validation accuracy: 96.875%\n",
            "Epoch number  8\n",
            "Training loss: 0.11558221489477005\n",
            "Training accuracy: 96.09375%\n",
            "Validation loss: 0.13561732247471808\n",
            "Validation accuracy: 97.65625%\n",
            "Epoch number  9\n",
            "Training loss: 0.1168035694372171\n",
            "Training accuracy: 98.4375%\n",
            "Validation loss: 0.14592149015516043\n",
            "Validation accuracy: 96.484375%\n",
            "Epoch number  10\n",
            "Training loss: 0.1073585400210896\n",
            "Training accuracy: 99.21875%\n",
            "Validation loss: 0.1581190090626478\n",
            "Validation accuracy: 97.4609375%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}